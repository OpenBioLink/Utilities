{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import shutil\r\n",
    "import wget\r\n",
    "import bz2\r\n",
    "import sys\r\n",
    "import json\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import gzip\r\n",
    "import rdflib\r\n",
    "import zipfile\r\n",
    "from rdflib import Namespace\r\n",
    "from rdflib.term import URIRef\r\n",
    "from os.path import exists\r\n",
    "from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD\r\n",
    "from urllib.parse import quote\r\n",
    "\r\n",
    "#create this bar_progress method which is invoked automatically from wget\r\n",
    "def bar_progress(current, total, width=80):\r\n",
    "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\r\n",
    "  # Don't use print() as it will print in new line every time.\r\n",
    "  sys.stdout.write(\"\\r\" + progress_message)\r\n",
    "  sys.stdout.flush()\r\n",
    "\r\n",
    "  # Methods\r\n",
    "def read_nodes(lst):\r\n",
    "    nodes = set()\r\n",
    "    for path in lst:\r\n",
    "        content = None\r\n",
    "        with open(path, encoding=\"utf8\") as f:\r\n",
    "            content = f.readlines()\r\n",
    "        content = [x.strip() for x in content]\r\n",
    "\r\n",
    "        for line in content:\r\n",
    "            head,rel,tail = line.split(\"\\t\")\r\n",
    "            nodes.add(head)\r\n",
    "            nodes.add(tail)\r\n",
    "    return nodes\r\n",
    "\r\n",
    "def read_line(path, skip_first):\r\n",
    "    with open(path, encoding=\"utf8\") as infile:\r\n",
    "        c = 0\r\n",
    "        while True:\r\n",
    "            line = infile.readline()\r\n",
    "            if not line:\r\n",
    "                break\r\n",
    "            if c % 100000 == 0:\r\n",
    "                print(c)\r\n",
    "            c += 1\r\n",
    "            if skip_first and c == 0:\r\n",
    "                continue\r\n",
    "            yield line"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Path setup\r\n",
    "train_path = r\"cache\\train.txt\"\r\n",
    "test_path = r\"cache\\test.txt\"\r\n",
    "valid_path = r\"cache\\valid.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "\r\n",
    "if not exists('cache'):\r\n",
    "    os.makedirs('cache')\r\n",
    "\r\n",
    "if not exists('cache/biokg.zip'):\r\n",
    "    url = \"http://snap.stanford.edu/ogb/data/linkproppred/biokg.zip\"\r\n",
    "    wget.download(url, 'cache/biokg.zip', bar=bar_progress)\r\n",
    "    import zipfile\r\n",
    "    with zipfile.ZipFile('cache/biokg.zip', 'r') as zip_ref:\r\n",
    "        zip_ref.extractall('cache')\r\n",
    "\r\n",
    "if not exists('cache/data.zip'):\r\n",
    "    url = \"https://github.com/OpenBioLink/Utilities/raw/main/data/Pheknowlator/data.zip\"\r\n",
    "    wget.download(url, 'cache/data.zip', bar=bar_progress)\r\n",
    "    import zipfile\r\n",
    "    with zipfile.ZipFile('cache/data.zip', 'r') as zip_ref:\r\n",
    "        zip_ref.extractall('cache')\r\n",
    "    \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading: 100% [963312546 / 963312546] bytes"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ontology = \"gene\"\r\n",
    "step_size = 500\r\n",
    "\r\n",
    "results = dict()\r\n",
    "count = 0\r\n",
    "ids_str = \"\"\r\n",
    "for x in tqdm(ncbigenes):\r\n",
    "    ids_str = ids_str + str(x) + \",\"\r\n",
    "    count = count + 1\r\n",
    "    if count >= step_size:\r\n",
    "        ids_str = ids_str[0:-1]\r\n",
    "        response = requests.post(f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db={ontology}&rettype=docsum&retmode=json\", data={'id':f'{ids_str}'}, headers={'content-type': 'application/x-www-form-urlencoded'})\r\n",
    "        response_json = response.json()\r\n",
    "        for key,value in response_json[\"result\"].items():\r\n",
    "            if key != 'uids':\r\n",
    "                results[key] = value\r\n",
    "        ids_str = \"\"\r\n",
    "        count = 0\r\n",
    "\r\n",
    "if count > 0:\r\n",
    "    response = requests.post(f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db={ontology}&rettype=docsum&retmode=json\", data={'id':f'{ids_str}'}, headers={'content-type': 'application/x-www-form-urlencoded'})\r\n",
    "    response_json = response.json()\r\n",
    "    for key,value in response_json[\"result\"].items():\r\n",
    "            if key != 'uids':\r\n",
    "                results[key] = value\r\n",
    "\r\n",
    "for x in results:\r\n",
    "    labels_file.write(\"NCBIGENE:\" + x + \"\\t\" + results[x][\"description\"] + \"\\n\")\r\n",
    "    if results[x][\"summary\"] != \"\":\r\n",
    "        descr_file.write(\"NCBIGENE:\" + x + \"\\t\" + results[x][\"summary\"] + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out = rdflib.Graph()\r\n",
    "out.bind(\"rdf\", RDF)\r\n",
    "out.bind(\"rdfs\", RDFS)\r\n",
    "ai = Namespace(\"http://ai-strategies.org/ns/\")\r\n",
    "out.bind(\"ai\", ai)\r\n",
    "\r\n",
    "g = rdflib.Graph()\r\n",
    "\r\n",
    "g.bind(\"ai\", ai)\r\n",
    "g.bind(\"rdf\", RDF)\r\n",
    "g.bind(\"rdfs\", RDFS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for type_ in [\"disease\", \"drug\", \"function\", \"protein\", \"sideeffect\"]:\r\n",
    "    with gzip.open(f'cache/mapping/{type_}_entidx2name.csv.gz', 'rb') as f:\r\n",
    "        content = f.read()\r\n",
    "    content = [x.strip() for x in content[1:]]\r\n",
    "    for line in content:\r\n",
    "        id, rel_label = line.split(\",\")\r\n",
    "        id = f\"{type_}:{id}\"\r\n",
    "\r\n",
    "        name = ?\r\n",
    "        url = ?\r\n",
    "        description = ?\r\n",
    "        # TODO Name descr ...\r\n",
    "\r\n",
    "        g.add((\r\n",
    "            ai.term(quote(id)),\r\n",
    "            RDFS.label,\r\n",
    "            rdflib.Literal(, datatype=XSD.string)\r\n",
    "\r\n",
    "        ))\r\n",
    "        g.add((\r\n",
    "            ai.term(quote(id)),\r\n",
    "            RDF.type,\r\n",
    "            rdflib.Literal(type_, datatype=XSD.string)\r\n",
    "        ))\r\n",
    "        if \"url\" in node[\"data\"]:\r\n",
    "            g.add((\r\n",
    "                ai.term(quote(id)),\r\n",
    "                ai.wwwresource,\r\n",
    "                rdflib.Literal(node[\"data\"][\"url\"], datatype=XSD.string)\r\n",
    "            ))\r\n",
    "        if \"description\" in node[\"data\"]:\r\n",
    "            g.add((\r\n",
    "                ai.term(quote(id)),\r\n",
    "                RDFS.comment,\r\n",
    "                rdflib.Literal(node[\"data\"][\"description\"], datatype=XSD.string)\r\n",
    "            ))\r\n",
    "\r\n",
    "\r\n",
    "# Relations\r\n",
    "with gzip.open('cache/mapping/relidx2relname.csv.gz', 'rb') as f:\r\n",
    "    content = f.read()\r\n",
    "content = [x.strip() for x in content[1:]]\r\n",
    "for line in content:\r\n",
    "    id, rel_label = line.split(\",\")\r\n",
    "    g.add((\r\n",
    "        ai.term(quote(id)),\r\n",
    "        RDFS.label,\r\n",
    "        rdflib.Literal(rel_label, datatype=XSD.string)\r\n",
    "    ))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "6034a6f5458e8007054679e0d1bec28e1df169f2230ff36b046fe759a4786f17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}